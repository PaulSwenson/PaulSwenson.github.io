<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Paul Swenson" />


<title>Case Study 2</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Paul's Website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="case_study_2.html">Case Study 2</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Case Study 2</h1>
<h4 class="author">Paul Swenson</h4>
<h4 class="date">3/4/2020</h4>

</div>


<p>#Libraries</p>
<pre class="r"><code>library(dplyr)     # for pipe</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(tidyverse) # for dropna</code></pre>
<pre><code>## -- Attaching packages ------------------------------------------------------------------------------------ tidyverse 1.2.1 --</code></pre>
<pre><code>## v ggplot2 3.2.1     v readr   1.3.1
## v tibble  2.1.3     v purrr   0.3.3
## v tidyr   1.0.0     v stringr 1.4.0
## v ggplot2 3.2.1     v forcats 0.4.0</code></pre>
<pre><code>## -- Conflicts --------------------------------------------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(class)     # for knn
library(caret)     # for confusion matrix</code></pre>
<pre><code>## Warning: package &#39;caret&#39; was built under R version 3.6.2</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<pre class="r"><code>library(ggplot2)   # for plotting
library(dplyr)
library(GGally)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<pre><code>## 
## Attaching package: &#39;GGally&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     nasa</code></pre>
<pre class="r"><code>library(reshape2)  # for melt</code></pre>
<pre><code>## 
## Attaching package: &#39;reshape2&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     smiths</code></pre>
<pre class="r"><code>library(MASS)      # for stepAIC</code></pre>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<pre class="r"><code>library(e1071)   # for naiveBayes</code></pre>
<pre><code>## Warning: package &#39;e1071&#39; was built under R version 3.6.2</code></pre>
<pre class="r"><code>library(jtools)    # for plot_summs</code></pre>
<pre><code>## Warning: package &#39;jtools&#39; was built under R version 3.6.3</code></pre>
<p>#Import data</p>
<pre class="r"><code>data = read.csv(&quot;./CaseStudy2-data.csv&quot;)
# Note: the attrition column was converted to 0 for No and 1 for Yes
#head(data)</code></pre>
<p>#Histogram Colored (blue and red)</p>
<pre class="r"><code>data[2] &lt;- lapply(data[2], as.numeric)
data[5] &lt;- lapply(data[5], as.numeric)
data[7] &lt;- lapply(data[7], as.numeric)
data[8] &lt;- lapply(data[8], as.numeric)
data[10] &lt;- lapply(data[10], as.numeric)
data[11] &lt;- lapply(data[11], as.numeric)
data[12] &lt;- lapply(data[12], as.numeric)
data[14] &lt;- lapply(data[14], as.numeric)
data[15] &lt;- lapply(data[15], as.numeric)
data[16] &lt;- lapply(data[16], as.numeric)
data[18] &lt;- lapply(data[18], as.numeric)
data[20] &lt;- lapply(data[20], as.numeric)
data[21] &lt;- lapply(data[21], as.numeric)
data[22] &lt;- lapply(data[22], as.numeric)
data[25,] &lt;- lapply(data[25,], as.numeric)</code></pre>
<pre><code>## Warning in `[&lt;-.factor`(`*tmp*`, iseq, value = 3): invalid factor level, NA
## generated

## Warning in `[&lt;-.factor`(`*tmp*`, iseq, value = 3): invalid factor level, NA
## generated

## Warning in `[&lt;-.factor`(`*tmp*`, iseq, value = 3): invalid factor level, NA
## generated

## Warning in `[&lt;-.factor`(`*tmp*`, iseq, value = 3): invalid factor level, NA
## generated

## Warning in `[&lt;-.factor`(`*tmp*`, iseq, value = 3): invalid factor level, NA
## generated

## Warning in `[&lt;-.factor`(`*tmp*`, iseq, value = 3): invalid factor level, NA
## generated

## Warning in `[&lt;-.factor`(`*tmp*`, iseq, value = 3): invalid factor level, NA
## generated</code></pre>
<pre class="r"><code>left &lt;- data %&gt;% filter(Attrition != &quot;No&quot;)
stayed &lt;- data %&gt;% filter(Attrition == &quot;No&quot;)

#for(col in 2:ncol(data))
#{
#  if(is.numeric(data[,col]))
#  {
#    hist(left[,col], col=rgb(1,0,0,0.5), main=col, xlab=col)
#    hist(stayed[,col], col=rgb(0,0,1,0.5), add=T)
#  }
#}

#TODO: remove NA values when plotting
plt &lt;- ggplot(data, aes(x=JobRole, y=MonthlyIncome)) + 
  geom_boxplot()
plt + theme(axis.text.x = element_text(angle = 60, hjust = 1))</code></pre>
<p><img src="case_study_2_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>#TODO: count the number of employees in each role</code></pre>
<div id="more-eda" class="section level1">
<h1>More EDA</h1>
<pre class="r"><code># I think these fields might be influential:
#age
#daily rate
#distance from home
#gender
#marital status
#num companies worked
#performance rating
#years since last promotion
#years at company

# Lets see what these columns look like
summary(data$Age)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   18.00   30.00   35.00   36.83   43.00   60.00</code></pre>
<pre class="r"><code>#table(is.na(data$Age))
#table(is.na(data$DailyRate))
#table(is.na(data$DistanceFromHome))
#table(is.na(data$Gender))                    #one missing
#table(is.na(data$MaritalStatus))             #one missing
#table(is.na(data$NumCompaniesWorked))
#table(is.na(data$PerformanceRating))
#table(is.na(data$YearsSinceLastPromotion))
#table(is.na(data$YearsAtCompany))</code></pre>
</div>
<div id="helper-functions" class="section level1">
<h1>Helper functions</h1>
<pre class="r"><code>normalize&lt;-function(y) 
{
  x&lt;-y[!is.na(y)]

  x&lt;-(x - min(x)) / (max(x) - min(x))

  y[!is.na(y)]&lt;-x

  return(y)
}</code></pre>
</div>
<div id="knn-classification" class="section level1">
<h1>KNN Classification</h1>
<pre class="r"><code># original columns used
#c(&quot;Attrition&quot;, &quot;Age&quot;, &quot;DailyRate&quot;, &quot;DistanceFromHome&quot;, &quot;Education&quot;, &quot;EnvironmentSatisfaction&quot;, &quot;HourlyRate&quot;, &quot;JobInvolvement&quot;, &quot;JobLevel&quot;, &quot;JobSatisfaction&quot;, &quot;RelationshipSatisfaction&quot;, &quot;YearsSinceLastPromotion&quot;, &quot;TotalWorkingYears&quot;, &quot;MonthlyRate&quot;)

selected_columns = c(&quot;Attrition&quot;, &quot;MonthlyRate&quot;, &quot;JobSatisfaction&quot;)

# 70% training data 30% test data
split_perc = 0.7
set.seed(78901)

# select only the used columns
knn_data &lt;- data %&gt;% drop_na %&gt;% dplyr::select( selected_columns ) %&gt;% apply(2, normalize) %&gt;% data.frame()

# split the dataset into training and testing datasets 
trainIndices = sample(1:dim(knn_data)[1],round(split_perc * dim(knn_data)[1]))
train_data = knn_data[trainIndices,]
test_data = knn_data[-trainIndices,]

# typical k value should be sqrt of the number of samples in your dataset
k_value = ceiling(sqrt(length(train_data$Attrition)))

# create scatterplot with labels
#knn_data_dropna %&gt;% ggplot(aes(x=YearsAtCompany, DailyRate, color=Attrition)) +
#  geom_point()+ggtitle(&quot;Years With Company vs Daily Rate&quot;)

# convert all columns to numeric values

# create knn model and evaluate the fit
classifications = knn(train_data[, selected_columns[-1]], test_data[, selected_columns[-1]], train_data$Attrition, prob = TRUE, k = k_value)
class_table &lt;- table(classifications,test_data$Attrition)
confusionMatrix(class_table)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##                
## classifications   0   1
##               0 217  44
##               1   0   0
##                                           
##                Accuracy : 0.8314          
##                  95% CI : (0.7804, 0.8748)
##     No Information Rate : 0.8314          
##     P-Value [Acc &gt; NIR] : 0.5401          
##                                           
##                   Kappa : 0               
##                                           
##  Mcnemar&#39;s Test P-Value : 9.022e-11       
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.8314          
##          Neg Pred Value :    NaN          
##              Prevalence : 0.8314          
##          Detection Rate : 0.8314          
##    Detection Prevalence : 1.0000          
##       Balanced Accuracy : 0.5000          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<pre class="r"><code># not a good fit... Let&#39;s try another model</code></pre>
<p>#Naiive Bayes Classification</p>
<pre class="r"><code># 70% training data 30% test data
split_perc = 0.7
set.seed(789011)

# remove na values
nb_data &lt;- data %&gt;% drop_na

# view all probabilities of classes
all.nb &lt;-naiveBayes(data=data, Attrition ~ .)
all.nb</code></pre>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##         0         1 
## 0.8390805 0.1609195 
## 
## Conditional probabilities:
##    ID
## Y       [,1]     [,2]
##   0 430.3014 251.3245
##   1 462.6071 250.2665
## 
##    Age
## Y       [,1]     [,2]
##   0 37.41233 8.673382
##   1 33.78571 9.614726
## 
##    BusinessTravel
## Y   Non-Travel Travel_Frequently Travel_Rarely
##   0 0.11385460        0.16872428    0.71742112
##   1 0.07857143        0.25000000    0.67142857
## 
##    DailyRate
## Y       [,1]     [,2]
##   0 821.1603 401.4137
##   1 784.2929 399.5637
## 
##    Department
## Y   Human Resources Research &amp; Development      Sales
##   0      0.03978052             0.66666667 0.29355281
##   1      0.04285714             0.53571429 0.42142857
## 
##    DistanceFromHome
## Y        [,1]     [,2]
##   0  9.028767 7.982869
##   1 10.957143 8.748995
## 
##    Education
## Y       [,1]     [,2]
##   0 2.923288 1.024865
##   1 2.785714 1.009207
## 
##    EducationField
## Y   Human Resources Life Sciences  Marketing    Medical      Other
##   0      0.01508916    0.41700960 0.10973937 0.31961591 0.05898491
##   1      0.02857143    0.37857143 0.14285714 0.26428571 0.06428571
##    EducationField
## Y   Technical Degree
##   0       0.07956104
##   1       0.12142857
## 
##    EmployeeCount
## Y   [,1] [,2]
##   0    1    0
##   1    1    0
## 
##    EmployeeNumber
## Y        [,1]     [,2]
##   0 1035.8658 606.5168
##   1  998.3714 596.8576
## 
##    EnvironmentSatisfaction
## Y       [,1]     [,2]
##   0 2.738356 1.077915
##   1 2.507143 1.190468
## 
##    Gender
## Y      Female      Male
##   0 0.4115226 0.5884774
##   1 0.3785714 0.6214286
## 
##    HourlyRate
## Y       [,1]     [,2]
##   0 65.29178 20.20311
##   1 67.29286 19.71214
## 
##    JobInvolvement
## Y       [,1]      [,2]
##   0 2.780822 0.6655665
##   1 2.421429 0.8141541
## 
##    JobLevel
## Y       [,1]      [,2]
##   0 2.116438 1.0943819
##   1 1.635714 0.9760493
## 
##    JobRole
## Y   Healthcare Representative Human Resources Laboratory Technician
##   0               0.091906722     0.028806584           0.168724280
##   1               0.057142857     0.042857143           0.214285714
##    JobRole
## Y       Manager Manufacturing Director Research Director
##   0 0.064471879            0.116598080       0.068587106
##   1 0.028571429            0.014285714       0.007142857
##    JobRole
## Y   Research Scientist Sales Executive Sales Representative
##   0        0.192043896     0.229080933          0.039780521
##   1        0.228571429     0.235714286          0.171428571
## 
##    JobSatisfaction
## Y       [,1]     [,2]
##   0 2.761644 1.111436
##   1 2.435714 1.094201
## 
##    MaritalStatus
## Y     Divorced    Married     Single
##   0 0.24554184 0.48285322 0.27160494
##   1 0.08571429 0.41428571 0.50000000
## 
##    MonthlyIncome
## Y       [,1]     [,2]
##   0 6702.000 4675.472
##   1 4764.786 3786.389
## 
##    MonthlyRate
## Y       [,1]     [,2]
##   0 14460.12 7126.983
##   1 13624.29 6993.816
## 
##    NumCompaniesWorked
## Y       [,1]     [,2]
##   0 2.660274 2.465606
##   1 3.078571 2.772080
## 
##    Over18
## Y   Y
##   0 1
##   1 1
## 
##    OverTime
## Y        [,1]      [,2]
##   0 0.2356164 0.4246744
##   1 0.5714286 0.4966486
## 
##    PercentSalaryHike
## Y       [,1]     [,2]
##   0 15.17534 3.627277
##   1 15.32857 3.928210
## 
##    PerformanceRating
## Y       [,1]      [,2]
##   0 3.149315 0.3566431
##   1 3.164286 0.3718651
## 
##    RelationshipSatisfaction
## Y       [,1]     [,2]
##   0 2.726027 1.090680
##   1 2.607143 1.161099
## 
##    StandardHours
## Y   [,1] [,2]
##   0   80    0
##   1   80    0
## 
##    StockOptionLevel
## Y        [,1]      [,2]
##   0 0.8397260 0.8382554
##   1 0.4928571 0.9016087
## 
##    TotalWorkingYears
## Y        [,1]     [,2]
##   0 11.602740 7.458968
##   1  8.185714 7.161634
## 
##    TrainingTimesLastYear
## Y       [,1]     [,2]
##   0 2.867123 1.277703
##   1 2.650000 1.234545
## 
##    WorkLifeBalance
## Y       [,1]      [,2]
##   0 2.809589 0.6874665
##   1 2.635714 0.8154155
## 
##    YearsAtCompany
## Y       [,1]     [,2]
##   0 7.301370 5.936068
##   1 5.192857 6.171292
## 
##    YearsInCurrentRole
## Y       [,1]     [,2]
##   0 4.453425 3.644888
##   1 2.907143 3.332630
## 
##    YearsSinceLastPromotion
## Y       [,1]     [,2]
##   0 2.175342 3.146526
##   1 2.135714 3.395322
## 
##    YearsWithCurrManager
## Y       [,1]     [,2]
##   0 4.369863 3.590900
##   1 2.942857 3.244855</code></pre>
<pre class="r"><code># looking at these values, the ones that stand out to me are:
# stock options, marital status, overtime, and job level

#remove columns
keep_columns = c(&quot;Attrition&quot;, &quot;OverTime&quot;, &quot;StockOptionLevel&quot;, &quot;MaritalStatus&quot;)

# select columns
#data_new &lt;- data %&gt;% select(selected_columns)
data_dropped &lt;- nb_data %&gt;% dplyr::select(keep_columns)
data_dropped$Attrition &lt;- as.factor(data_dropped$Attrition)

# split the dataset into training and testing datasets 
trainIndices = sample(1:dim(data_dropped)[1],round(split_perc * dim(data_dropped)[1]))
train_data = data_dropped[trainIndices,]
test_data = data_dropped[-trainIndices,]


# Train with NaiveBayes
train_data.nb &lt;- naiveBayes(data=train_data, Attrition ~ .)
pred &lt;- predict(train_data.nb, test_data)
confusionMatrix(pred, test_data$Attrition)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 207  28
##          1  12  14
##                                           
##                Accuracy : 0.8467          
##                  95% CI : (0.7972, 0.8882)
##     No Information Rate : 0.8391          
##     P-Value [Acc &gt; NIR] : 0.40725         
##                                           
##                   Kappa : 0.3292          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.01771         
##                                           
##             Sensitivity : 0.9452          
##             Specificity : 0.3333          
##          Pos Pred Value : 0.8809          
##          Neg Pred Value : 0.5385          
##              Prevalence : 0.8391          
##          Detection Rate : 0.7931          
##    Detection Prevalence : 0.9004          
##       Balanced Accuracy : 0.6393          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<pre class="r"><code># Naive Bayes using caret
attr_data &lt;- data_dropped$Attrition
predictor_data &lt;- data_dropped %&gt;% dplyr::select(-c(&quot;Attrition&quot;))
train_data.cnb &lt;- train(predictor_data, attr_data, &#39;nb&#39;, ,trControl=trainControl(method=&#39;cv&#39;,number=10))

# check individual variable contributions. I&#39;ll only keep the top 5-10
#plot(varImp(train_data.cnb, scale = FALSE))

# summary of fit
#summary(train_data.cnb$results)</code></pre>
</div>
<div id="assess-relationships-to-income" class="section level1">
<h1>Assess Relationships to income</h1>
<pre class="r"><code># first look at relationships between the data and assess linearity
#data.m &lt;- melt(data, &quot;MonthlyRate&quot;)

#ggplot(data.m, aes(value, MonthlyRate)) + 
#  geom_line() + 
#  facet_wrap(~variable, scales = &quot;free&quot;)</code></pre>
<p>#Regression</p>
<pre class="r"><code>#selected_columns = c(&quot;MonthlyIncome&quot;, &quot;Age&quot;, &quot;Education&quot;, &quot;EnvironmentSatisfaction&quot;, &quot;JobInvolvement&quot;, &quot;JobLevel&quot;, &quot;JobSatisfaction&quot;, &quot;RelationshipSatisfaction&quot;, &quot;YearsSinceLastPromotion&quot;, &quot;TotalWorkingYears&quot;, &quot;PerformanceRating&quot;, &quot;TotalWorkingYears&quot;, &quot;YearsAtCompany&quot;, &quot;YearsInCurrentRole&quot;, &quot;Department&quot;)

drop_columns = c(&quot;Over18&quot;, &quot;StandardHours&quot;, &quot;MonthlyRate&quot;, &quot;HourlyRate&quot;)

# select columns
#data_new &lt;- data %&gt;% select(selected_columns)
test_data_dropped &lt;- dplyr::select(data, -drop_columns)

# split the dataset into training and testing datasets 
trainIndices = sample(1:dim(test_data_dropped)[1],round(split_perc * dim(test_data_dropped)[1]))
lm_train_data = test_data_dropped[trainIndices,]
lm_test_data = test_data_dropped[-trainIndices,]

# generate the full model
train.full_lm &lt;- lm(MonthlyIncome ~., data=lm_train_data)

# The stepwise model performs much better than this one
#summary(train.full_lm)

# generate the stepwise model
step.model &lt;- stepAIC(train.full_lm, direction = &quot;both&quot;, trace = FALSE)
summary(step.model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = MonthlyIncome ~ ID + BusinessTravel + JobInvolvement + 
##     JobLevel + JobRole + TotalWorkingYears + YearsWithCurrManager, 
##     data = lm_train_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3602.0  -632.1   -19.5   569.9  4094.9 
## 
## Coefficients:
##                                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                     -426.1063   338.0878  -1.260  0.20804    
## ID                                -0.3265     0.1718  -1.901  0.05785 .  
## BusinessTravelTravel_Frequently  101.9949   168.3147   0.606  0.54476    
## BusinessTravelTravel_Rarely      370.5055   142.5556   2.599  0.00958 ** 
## JobInvolvement                   111.8741    63.9120   1.750  0.08056 .  
## JobLevel                        2757.3419    95.3058  28.932  &lt; 2e-16 ***
## JobRoleHuman Resources          -420.5238   319.0107  -1.318  0.18794    
## JobRoleLaboratory Technician    -626.6488   198.9892  -3.149  0.00172 ** 
## JobRoleManager                  4154.4125   266.6881  15.578  &lt; 2e-16 ***
## JobRoleManufacturing Director    301.8201   201.4164   1.498  0.13454    
## JobRoleResearch Director        4352.8667   257.5241  16.903  &lt; 2e-16 ***
## JobRoleResearch Scientist       -400.2296   198.9529  -2.012  0.04471 *  
## JobRoleSales Executive           -19.8971   171.5248  -0.116  0.90769    
## JobRoleSales Representative     -325.4095   245.9715  -1.323  0.18636    
## TotalWorkingYears                 53.9011     9.7504   5.528 4.86e-08 ***
## YearsWithCurrManager             -35.6702    13.8585  -2.574  0.01030 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1052 on 592 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.9505, Adjusted R-squared:  0.9492 
## F-statistic: 757.3 on 15 and 592 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># I&#39;m going to remove the less significant columns:
#   Business Travel, Daily Rate (shouldn&#39;t be used anyways), 
lm_train_data &lt;- dplyr::select(lm_train_data, -c( &quot;BusinessTravel&quot;, &quot;DailyRate&quot; ))

# generate the full model
train.full_lm &lt;- lm(MonthlyIncome ~., data=lm_train_data)
summary(train.full_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = MonthlyIncome ~ ., data = lm_train_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3716.8  -664.4     4.2   594.5  3977.6 
## 
## Coefficients: (1 not defined because of singularities)
##                                    Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                       -38.22875  942.24743  -0.041 0.967652
## ID                                 -0.38144    0.17957  -2.124 0.034090
## Age                                -2.98208    7.17436  -0.416 0.677818
## Attrition                         120.51198  138.33024   0.871 0.384020
## DepartmentResearch &amp; Development  298.25082  546.36675   0.546 0.585363
## DepartmentSales                  -366.17464  556.29004  -0.658 0.510649
## DistanceFromHome                   -4.74744    5.56965  -0.852 0.394365
## Education                         -38.79610   45.50435  -0.853 0.394253
## EducationFieldLife Sciences       268.98919  489.21662   0.550 0.582648
## EducationFieldMarketing           269.29063  512.49209   0.525 0.599474
## EducationFieldMedical             187.68497  492.15249   0.381 0.703083
## EducationFieldOther               441.93490  514.69010   0.859 0.390901
## EducationFieldTechnical Degree    220.95741  507.23693   0.436 0.663286
## EmployeeCount                            NA         NA      NA       NA
## EmployeeNumber                      0.10262    0.07399   1.387 0.165999
## EnvironmentSatisfaction           -31.44409   40.94413  -0.768 0.442822
## GenderMale                         18.66589   90.78543   0.206 0.837174
## JobInvolvement                    142.89534   67.09096   2.130 0.033613
## JobLevel                         2720.27023   99.72437  27.278  &lt; 2e-16
## JobRoleHuman Resources            -23.18032  625.02066  -0.037 0.970428
## JobRoleLaboratory Technician     -640.91015  205.70560  -3.116 0.001928
## JobRoleManager                   4546.64629  336.66232  13.505  &lt; 2e-16
## JobRoleManufacturing Director     408.79781  208.52485   1.960 0.050436
## JobRoleResearch Director         4451.63877  269.02079  16.548  &lt; 2e-16
## JobRoleResearch Scientist        -432.58188  205.06491  -2.109 0.035341
## JobRoleSales Executive            672.65024  426.85376   1.576 0.115623
## JobRoleSales Representative       271.89063  462.61736   0.588 0.556953
## JobSatisfaction                     7.01809   40.00446   0.175 0.860802
## MaritalStatusMarried              121.63848  122.38931   0.994 0.320713
## MaritalStatusSingle               109.16928  165.35567   0.660 0.509388
## NumCompaniesWorked                 23.11213   20.27383   1.140 0.254769
## OverTime                           13.16271  101.93640   0.129 0.897303
## PercentSalaryHike                  13.99157   19.27820   0.726 0.468279
## PerformanceRating                -237.63790  197.93278  -1.201 0.230409
## RelationshipSatisfaction           18.64618   41.24845   0.452 0.651409
## StockOptionLevel                   27.27390   70.55452   0.387 0.699224
## TotalWorkingYears                  52.58469   13.46885   3.904 0.000106
## TrainingTimesLastYear              -5.61979   35.69075  -0.157 0.874940
## WorkLifeBalance                   -60.61594   64.35069  -0.942 0.346614
## YearsAtCompany                     19.47530   16.07898   1.211 0.226314
## YearsInCurrentRole                 -8.98316   20.50642  -0.438 0.661506
## YearsSinceLastPromotion             6.99822   18.64579   0.375 0.707560
## YearsWithCurrManager              -51.37747   20.00223  -2.569 0.010467
##                                     
## (Intercept)                         
## ID                               *  
## Age                                 
## Attrition                           
## DepartmentResearch &amp; Development    
## DepartmentSales                     
## DistanceFromHome                    
## Education                           
## EducationFieldLife Sciences         
## EducationFieldMarketing             
## EducationFieldMedical               
## EducationFieldOther                 
## EducationFieldTechnical Degree      
## EmployeeCount                       
## EmployeeNumber                      
## EnvironmentSatisfaction             
## GenderMale                          
## JobInvolvement                   *  
## JobLevel                         ***
## JobRoleHuman Resources              
## JobRoleLaboratory Technician     ** 
## JobRoleManager                   ***
## JobRoleManufacturing Director    .  
## JobRoleResearch Director         ***
## JobRoleResearch Scientist        *  
## JobRoleSales Executive              
## JobRoleSales Representative         
## JobSatisfaction                     
## MaritalStatusMarried                
## MaritalStatusSingle                 
## NumCompaniesWorked                  
## OverTime                            
## PercentSalaryHike                   
## PerformanceRating                   
## RelationshipSatisfaction            
## StockOptionLevel                    
## TotalWorkingYears                ***
## TrainingTimesLastYear               
## WorkLifeBalance                     
## YearsAtCompany                      
## YearsInCurrentRole                  
## YearsSinceLastPromotion             
## YearsWithCurrManager             *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1070 on 566 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.951,  Adjusted R-squared:  0.9474 
## F-statistic: 267.9 on 41 and 566 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># generate the stepwise model
step.model &lt;- stepAIC(train.full_lm, direction = &quot;both&quot;, trace = FALSE)
summary(step.model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = MonthlyIncome ~ ID + EmployeeNumber + JobInvolvement + 
##     JobLevel + JobRole + TotalWorkingYears + YearsWithCurrManager, 
##     data = lm_train_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3865.9  -623.4    -5.9   618.3  4079.8 
## 
## Coefficients:
##                                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                   -272.80397  333.41777  -0.818  0.41357    
## ID                              -0.32907    0.17267  -1.906  0.05717 .  
## EmployeeNumber                   0.10810    0.07162   1.509  0.13174    
## JobInvolvement                 117.01265   64.07669   1.826  0.06833 .  
## JobLevel                      2753.56971   95.64537  28.789  &lt; 2e-16 ***
## JobRoleHuman Resources        -461.69310  323.04342  -1.429  0.15347    
## JobRoleLaboratory Technician  -606.64837  200.11771  -3.031  0.00254 ** 
## JobRoleManager                4169.99956  267.63314  15.581  &lt; 2e-16 ***
## JobRoleManufacturing Director  339.68364  202.36300   1.679  0.09376 .  
## JobRoleResearch Director      4379.09361  258.69720  16.927  &lt; 2e-16 ***
## JobRoleResearch Scientist     -400.44193  199.98830  -2.002  0.04570 *  
## JobRoleSales Executive          -3.21350  172.48023  -0.019  0.98514    
## JobRoleSales Representative   -341.07499  246.07526  -1.386  0.16625    
## TotalWorkingYears               55.37161    9.79757   5.652 2.47e-08 ***
## YearsWithCurrManager           -39.64818   13.88402  -2.856  0.00444 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1058 on 593 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.9498, Adjusted R-squared:  0.9486 
## F-statistic: 800.8 on 14 and 593 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># calculate the RMSE
lm_test_data$PredictedMonthlyIncome &lt;- predict(step.model, lm_test_data)
pred_actual &lt;- lm_test_data %&gt;% dplyr::select( c(&quot;MonthlyIncome&quot;, &quot;PredictedMonthlyIncome&quot;, &quot;ID&quot;) )
RMSE(pred_actual$PredictedMonthlyIncome, pred_actual$MonthlyIncome, na.rm=TRUE)</code></pre>
<pre><code>## [1] 1092.36</code></pre>
<pre class="r"><code>step.model$coefficients</code></pre>
<pre><code>##                   (Intercept)                            ID 
##                  -272.8039665                    -0.3290658 
##                EmployeeNumber                JobInvolvement 
##                     0.1081024                   117.0126511 
##                      JobLevel        JobRoleHuman Resources 
##                  2753.5697113                  -461.6930994 
##  JobRoleLaboratory Technician                JobRoleManager 
##                  -606.6483707                  4169.9995608 
## JobRoleManufacturing Director      JobRoleResearch Director 
##                   339.6836417                  4379.0936089 
##     JobRoleResearch Scientist        JobRoleSales Executive 
##                  -400.4419279                    -3.2135035 
##   JobRoleSales Representative             TotalWorkingYears 
##                  -341.0749903                    55.3716080 
##          YearsWithCurrManager 
##                   -39.6481800</code></pre>
<pre class="r"><code>plot_summs(step.model)</code></pre>
<p><img src="case_study_2_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="log-transformation" class="section level1">
<h1>Log transformation</h1>
<pre class="r"><code># try log transformation on the monthly income
# The original RMSE was lower by about $400, so we will use that one

#
# dead code below
#

#test_data_dropped$MonthlyIncome &lt;- log( test_data_dropped$MonthlyIncome )

#trainIndices = sample(1:dim(test_data_dropped)[1],round(split_perc * dim(test_data_dropped)[1]))
#lm_train_data = test_data_dropped[trainIndices,]
#lm_test_data = test_data_dropped[-trainIndices,]

# generate the full model
#train.full_lm &lt;- lm(MonthlyIncome ~., data=lm_train_data)
#summary(train.full_lm)

# generate the stepwise model
#step.model &lt;- stepAIC(train.full_lm, direction = &quot;both&quot;, trace = FALSE)
#summary(step.model)

#lm_test_data$PredictedMonthlyIncome &lt;- predict(step.model, lm_test_data)
#pred_actual &lt;- lm_test_data %&gt;% dplyr::select( c(&quot;MonthlyIncome&quot;, &quot;PredictedMonthlyIncome&quot;, &quot;ID&quot;) )
#RMSE(2.7183 ** pred_actual$PredictedMonthlyIncome, 2.7183 ** pred_actual$MonthlyIncome, na.rm=TRUE)</code></pre>
<pre class="r"><code>#TODO: check regression assumptions for the first model.</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
